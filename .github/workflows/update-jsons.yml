name: Update JSONs
on:
  pull_request:
  workflow_dispatch:
  schedule:
    - cron: '23 0 * * *'  # Daily at 00:23 UTC.
jobs:
  update-jsons:
    runs-on: ubuntu-latest
    steps:
      # See https://github.com/marketplace/actions/add-commit#working-with-prs for why two steps are needed.
      - name: Check out repository code (scheduled)
        uses: actions/checkout@8e5e7e5ab8b370d6c329ec480221332ada57f0ab  # pin@v3.5.2
        if: ${{ github.event_name != 'pull_request' }}

      - name: Check out repository code (PR)
        uses: actions/checkout@8e5e7e5ab8b370d6c329ec480221332ada57f0ab  # pin@v3.5.2
        if: ${{ github.event_name == 'pull_request' }}
        with:
          repository: ${{ github.event.pull_request.head.repo.full_name }}
          ref: ${{ github.event.pull_request.head.ref }}

      - name: Set up Python
        uses: actions/setup-python@57ded4d7d5e986d7296eab16560982c6dd7c923b  # pin@v4.6.0
        with:
          python-version: "3.10"

      - name: Set up cache
        uses: actions/cache@88522ab9f39a2ea568f7027eddc7d8d8bc9d59c8  # pin@v3.3.1
        id: cache
        with:
          path: |
            .venv
            ~/.poetry
            ~/.cache
          key: venv-${{ hashFiles('poetry.lock', '.pre-commit-config.yaml') }}

      - name: Install poetry
        run: pipx install poetry==$(cat .poetry-version)

      - name: Install dependencies
        run: |
          poetry config virtualenvs.in-project true
          poetry install

      - name: Download RLS survey data
        run: poetry run rls-data download-survey-data data/rls-surveys/

      # It's possible to extract the species data from species.json without scraping,
      # but this approach acts as a monitor that tracks changes to species.json and
      # verifies that they're reflected in pages that can be scraped.
      - name: Download species.json
        run: |
          curl -sS --compressed \
            "https://reeflifesurvey.com/species/api/species.json?v=$(date --utc +%s)" \
            | jq > output/species.json

      # Extra debugging support, as some of species.json originates from raw_species.csv.
      # Removing the first column (FID) because it changes on every request.
      - name: Download raw_species.csv
        run: |
          curl -sS --compressed \
            "https://geoserver-123.aodn.org.au/geoserver/imos/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=imos%3Aep_species_list_data&outputFormat=csv" \
            | cut -d, -f2- > output/raw_species.csv

      - name: Crawl RLS species data
        run: |
          poetry run scrapy runspider rls/scraper.py \
            --overwrite-output data/rls-site-crawl.json \
            --loglevel INFO \
            --set USER_AGENT="Scrapy-RLS-API (+https://reeflifesurvey.com/)"

      - name: Create API files
        run: poetry run rls-data create-api-jsons data/rls-site-crawl.json data/rls-surveys/ output-tmp/

      - name: Commit API files
        run: |
          mkdir -p output/
          cp output-tmp/*.json output/
          if [ -n "$(git status --porcelain)" ]; then
            git config user.name "GitHub Actions Bot"
            git config user.email "<>"
            git add output/
            git commit -m "Update API files"
            git push
          else
            echo "The API files haven't changed"
          fi
